{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img style=\"float: right\" src=\"images/surfsara.png\">\n",
    "<br/>\n",
    "<hr style=\"clear: both\" />\n",
    "\n",
    "# Spark Structured API: DataFrames and SQL\n",
    "In this notebook we will look at Spark's Structured API. We will see how you can use DataFrames and SQL to perform common data processing operations.\n",
    "\n",
    "First, we will need to create a `SparkSession` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `SparkSession` object is the entry point for our Spark cluster. We can use a `SparkSession` to create DataFrames, as we will soon see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames from Python collections\n",
    "We can create a DataFrame from an existing Python collection, such as a list or a dictionary. In addition to the collection itself we will also describe (part of) the structure of the data by naming the columns. Additionally, we can specify the data types of the columns. However, in this case we will let Spark infer this automatically.\n",
    "\n",
    "First, a list of tuples in Python is created, called `phone_stock`. Next, we create a list called `columns` that contain the name of all columns of the DataFrame. Then we use these two lists as input for [`createDataFrame`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession.createDataFrame). The result is the DataFrame `phone_df`. Next we print the type of both `phone_stock` and `phone_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of phone_stock: <class 'list'>\n",
      "the type of phone_df: <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "phone_stock = [\n",
    "    ('iPhone 6', 'Apple', 6, 549.00),\n",
    "    ('iPhone 6s', 'Apple', 5, 585.00),\n",
    "    ('iPhone 7', 'Apple', 11, 739.00),\n",
    "    ('Pixel', 'Google', 8, 859.00),\n",
    "    ('Pixel XL', 'Google', 2, 959.00),\n",
    "    ('Galaxy S7', 'Samsung', 10, 539.00),\n",
    "    ('Galaxy S6', 'Samsung', 5, 414.00),\n",
    "    ('Galaxy A5', 'Samsung', 7, 297.00),\n",
    "    ('Galaxy Note 7', 'Samsung', 0, 841.00)\n",
    "]\n",
    "\n",
    "columns = ['model', 'brand', 'stock', 'unit_price']\n",
    "\n",
    "phone_df = spark.createDataFrame(phone_stock, columns)\n",
    "\n",
    "print('the type of phone_stock: ' + str(type(phone_stock)))\n",
    "print('the type of phone_df: ' + str(type(phone_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `phone_stock` variable is a Python `list`, kept in memory. Using `createDataFrame`, it is converted into a DataFrame, which is now located on our Spark cluster. It is important to distinguish between data available locally (driver-side), such as `phone_stock`, and data available on the cluster (cluster-side), such as `phone_df`.<br>\n",
    "Although data can be downloaded from the cluster to the driver, and uploaded from the driver to the cluster, this is often a costly operation. In order to reduce computation time, it is important to know where our data 'lives', and how Spark operation affect the location of our data. We will come back to this issue later in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see a few rows of a DataFrame we use [`show()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.show). By default it shows 20 rows, but you can give the desired number of rows that you want to see as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----+----------+\n",
      "|    model| brand|stock|unit_price|\n",
      "+---------+------+-----+----------+\n",
      "| iPhone 6| Apple|    6|     549.0|\n",
      "|iPhone 6s| Apple|    5|     585.0|\n",
      "| iPhone 7| Apple|   11|     739.0|\n",
      "|    Pixel|Google|    8|     859.0|\n",
      "| Pixel XL|Google|    2|     959.0|\n",
      "+---------+------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phone_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`collect()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.collect) _action_ returns all data from a DataFrame to the driver. Notice that the result is a Python `list` containing `Row` objects. In turn, each `Row` object consists of pairs of column names and attribute values.\n",
    "\n",
    "**Note**: `collect` is a costly operation. Each executor in the cluster will return its part of the DataFrame to the driver in its entirety. This is fine for small DataFrames, such as on the one we are using, but can cause serious problems on larger DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(model='iPhone 6', brand='Apple', stock=6, unit_price=549.0),\n",
       " Row(model='iPhone 6s', brand='Apple', stock=5, unit_price=585.0),\n",
       " Row(model='iPhone 7', brand='Apple', stock=11, unit_price=739.0),\n",
       " Row(model='Pixel', brand='Google', stock=8, unit_price=859.0),\n",
       " Row(model='Pixel XL', brand='Google', stock=2, unit_price=959.0),\n",
       " Row(model='Galaxy S7', brand='Samsung', stock=10, unit_price=539.0),\n",
       " Row(model='Galaxy S6', brand='Samsung', stock=5, unit_price=414.0),\n",
       " Row(model='Galaxy A5', brand='Samsung', stock=7, unit_price=297.0),\n",
       " Row(model='Galaxy Note 7', brand='Samsung', stock=0, unit_price=841.0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_phones = phone_df.collect()\n",
    "all_phones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working directly with a list of row objects is cumbersome. To work directly with data on the driver's side, we usually convert the Spark DataFrame to a `pandas` DataFrame. [`pandas`](https://pandas.pydata.org/) is a data processing library that allows us to manipulate tabular table. It is suitable for processing that isn't too intensive and data that isn't too large to fit into local memory (otherwise, why would we want to use Spark?).\n",
    "\n",
    "Spark DataFrames have a [`toPandas()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.toPandas) action defined on them, that will pull all data to the driver and convert it to a `pandas` DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>brand</th>\n",
       "      <th>stock</th>\n",
       "      <th>unit_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iPhone 6</td>\n",
       "      <td>Apple</td>\n",
       "      <td>6</td>\n",
       "      <td>549.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iPhone 6s</td>\n",
       "      <td>Apple</td>\n",
       "      <td>5</td>\n",
       "      <td>585.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iPhone 7</td>\n",
       "      <td>Apple</td>\n",
       "      <td>11</td>\n",
       "      <td>739.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pixel</td>\n",
       "      <td>Google</td>\n",
       "      <td>8</td>\n",
       "      <td>859.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pixel XL</td>\n",
       "      <td>Google</td>\n",
       "      <td>2</td>\n",
       "      <td>959.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Galaxy S7</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>10</td>\n",
       "      <td>539.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Galaxy S6</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>5</td>\n",
       "      <td>414.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Galaxy A5</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>7</td>\n",
       "      <td>297.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Galaxy Note 7</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>0</td>\n",
       "      <td>841.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model    brand  stock  unit_price\n",
       "0       iPhone 6    Apple      6       549.0\n",
       "1      iPhone 6s    Apple      5       585.0\n",
       "2       iPhone 7    Apple     11       739.0\n",
       "3          Pixel   Google      8       859.0\n",
       "4       Pixel XL   Google      2       959.0\n",
       "5      Galaxy S7  Samsung     10       539.0\n",
       "6      Galaxy S6  Samsung      5       414.0\n",
       "7      Galaxy A5  Samsung      7       297.0\n",
       "8  Galaxy Note 7  Samsung      0       841.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several methods to inspect the structure of a DataFrame: `printSchema`, `schema` and `describe`. [`printSchema`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.printSchema) is especially useful with complicated nested structures, because it provides a human-readable form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- model: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- stock: long (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phone_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all columns are listed, together with their type and a boolean value that indicates whether the value for that column can be NULL. Although this DataFrame does not contain nested structures, we will see a more involved example later in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema's can also be listed programmatically. By calling [`schema`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.schema) we get to see the structure of the DataFrame in Spark's internal types. It is possible to define a schema in code by making use of these types, although we won't do this here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(model,StringType,true),StructField(brand,StringType,true),StructField(stock,LongType,true),StructField(unit_price,DoubleType,true)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `schema`'s `fields` attribute will provide a more readable way to inspect the DataFrame's structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(model,StringType,true),\n",
       " StructField(brand,StringType,true),\n",
       " StructField(stock,LongType,true),\n",
       " StructField(unit_price,DoubleType,true)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_df.schema.fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`describe`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe) will compute summary statistics for numeric and string columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>model</th>\n",
       "      <th>brand</th>\n",
       "      <th>stock</th>\n",
       "      <th>unit_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6.0</td>\n",
       "      <td>642.4444444444445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.5355339059327378</td>\n",
       "      <td>220.82295573100586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>Galaxy A5</td>\n",
       "      <td>Apple</td>\n",
       "      <td>0</td>\n",
       "      <td>297.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>iPhone 7</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>11</td>\n",
       "      <td>959.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary      model    brand               stock          unit_price\n",
       "0   count          9        9                   9                   9\n",
       "1    mean       None     None                 6.0   642.4444444444445\n",
       "2  stddev       None     None  3.5355339059327378  220.82295573100586\n",
       "3     min  Galaxy A5    Apple                   0               297.0\n",
       "4     max   iPhone 7  Samsung                  11               959.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data extraction\n",
    "\n",
    "Now that we have our data in a DataFrame, we want to use it to manipulate the data. Let's start by selecting subsets of the data: specific columns and/or rows.\n",
    "\n",
    "### Selecting columns\n",
    "\n",
    "Often we are not interested in all the columns of our data. DataFrames make it very easy to select a subset of the data by using the [`select`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe) method. Realise that we are not modifying the original DataFrame, but creating a new one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|        model|\n",
      "+-------------+\n",
      "|     iPhone 6|\n",
      "|    iPhone 6s|\n",
      "|     iPhone 7|\n",
      "|        Pixel|\n",
      "|     Pixel XL|\n",
      "|    Galaxy S7|\n",
      "|    Galaxy S6|\n",
      "|    Galaxy A5|\n",
      "|Galaxy Note 7|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only the model column\n",
    "model_df = phone_df.select('model')\n",
    "model_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also rename a column by using [`expr`](https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.functions.expr)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|  brand|      mymodel|\n",
      "+-------+-------------+\n",
      "|  Apple|     iPhone 6|\n",
      "|  Apple|    iPhone 6s|\n",
      "|  Apple|     iPhone 7|\n",
      "| Google|        Pixel|\n",
      "| Google|     Pixel XL|\n",
      "|Samsung|    Galaxy S7|\n",
      "|Samsung|    Galaxy S6|\n",
      "|Samsung|    Galaxy A5|\n",
      "|Samsung|Galaxy Note 7|\n",
      "+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "mymodel_df = phone_df.select('brand', expr('model as mymodel'))\n",
    "mymodel_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|  brand|        model|\n",
      "+-------+-------------+\n",
      "|  Apple|     iPhone 6|\n",
      "|  Apple|    iPhone 6s|\n",
      "|  Apple|     iPhone 7|\n",
      "| Google|        Pixel|\n",
      "| Google|     Pixel XL|\n",
      "|Samsung|    Galaxy S7|\n",
      "|Samsung|    Galaxy S6|\n",
      "|Samsung|    Galaxy A5|\n",
      "|Samsung|Galaxy Note 7|\n",
      "+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select both the brand and model columns\n",
    "bm_df = phone_df.select(['brand', 'model'])\n",
    "bm_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Columns specifications\n",
    "In the previous examples we have used various _column specifications_ for selecting data. For example, the following column specifications for selecting the `brand` and `model` columns are all equivalent:\n",
    "```\n",
    "bm_df = phone_df.select('brand', 'model')\n",
    "bm_df = phone_df.select(['brand', 'model'])\n",
    "bm_df = phone_df.select([phone_df.brand, phone_df.model])\n",
    "bm_df = phone_df.select(phone_df['brand'], phone_df['model'])\n",
    "```\n",
    "Although the first two are shorter, we will sometimes need to use the last two column specifications for more complex queries, and to resolve any ambiguities for Spark's parser. In fact, the Spark documentation states that the last column specification is preferred, although you are free to use the specification you see fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "Select the `model` and `stock` columns from `phone_df`, with whatever column specification you prefer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|        model|stock|\n",
      "+-------------+-----+\n",
      "|     iPhone 6|    6|\n",
      "|    iPhone 6s|    5|\n",
      "|     iPhone 7|   11|\n",
      "|        Pixel|    8|\n",
      "|     Pixel XL|    2|\n",
      "|    Galaxy S7|   10|\n",
      "|    Galaxy S6|    5|\n",
      "|    Galaxy A5|    7|\n",
      "|Galaxy Note 7|    0|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Select the model and stock columns\n",
    "ms_df = phone_df.select('model', 'stock')\n",
    "ms_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering rows\n",
    "\n",
    "We can filter specific rows by using the DataFrame [`filter`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.filter) method. Please note that the [`where`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.where) method is an alias for `filter`. As with the column specifications for the `select` method, there are several ways of filtering with the same query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+----------+\n",
      "|   model| brand|stock|unit_price|\n",
      "+--------+------+-----+----------+\n",
      "|   Pixel|Google|    8|     859.0|\n",
      "|Pixel XL|Google|    2|     959.0|\n",
      "+--------+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select rows with phones from Google\n",
    "google_df = phone_df.filter(phone_df['brand'] == 'Google')\n",
    "google_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+----------+\n",
      "|   model| brand|stock|unit_price|\n",
      "+--------+------+-----+----------+\n",
      "|   Pixel|Google|    8|     859.0|\n",
      "|Pixel XL|Google|    2|     959.0|\n",
      "+--------+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select rows with phones from Google, this time with a query string. Note the double quotes to specify a string\n",
    "google_df = phone_df.filter('brand = \"Google\"')\n",
    "google_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "Select the rows with `unit_price` less than 550.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-----+----------+\n",
      "|    model|  brand|stock|unit_price|\n",
      "+---------+-------+-----+----------+\n",
      "| iPhone 6|  Apple|    6|     549.0|\n",
      "|Galaxy S7|Samsung|   10|     539.0|\n",
      "|Galaxy S6|Samsung|    5|     414.0|\n",
      "|Galaxy A5|Samsung|    7|     297.0|\n",
      "+---------+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "cheap_df = phone_df.filter('unit_price < 550.00')\n",
    "cheap_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple filter conditions can be specified using Python's [bitwise operators](https://docs.python.org/3/library/stdtypes.html#bitwise-operations-on-integer-types), such as `|` (or) and `&` (and). Of course, we are not manipulating bits. Instead, think of the bitwise operators as applying a boolean operation on each pair of elements between two columns.\n",
    "\n",
    "Also, please be aware that due to Python's operator precedence rules, individual expressions must be wrapped in enclosing brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----+----------+\n",
      "|    model| brand|stock|unit_price|\n",
      "+---------+------+-----+----------+\n",
      "| iPhone 6| Apple|    6|     549.0|\n",
      "|iPhone 6s| Apple|    5|     585.0|\n",
      "| iPhone 7| Apple|   11|     739.0|\n",
      "|    Pixel|Google|    8|     859.0|\n",
      "| Pixel XL|Google|    2|     959.0|\n",
      "+---------+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phone_df.filter((phone_df.brand == 'Apple') | (phone_df.brand == 'Google')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordering rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the [`orderBy`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderBy) method to sort data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+-----+----------+\n",
      "|        model|  brand|stock|unit_price|\n",
      "+-------------+-------+-----+----------+\n",
      "|    Galaxy A5|Samsung|    7|     297.0|\n",
      "|    Galaxy S6|Samsung|    5|     414.0|\n",
      "|    Galaxy S7|Samsung|   10|     539.0|\n",
      "|     iPhone 6|  Apple|    6|     549.0|\n",
      "|    iPhone 6s|  Apple|    5|     585.0|\n",
      "|     iPhone 7|  Apple|   11|     739.0|\n",
      "|Galaxy Note 7|Samsung|    0|     841.0|\n",
      "|        Pixel| Google|    8|     859.0|\n",
      "|     Pixel XL| Google|    2|     959.0|\n",
      "+-------------+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phone_df.orderBy('unit_price').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we use a chain of DataFrame methods that are very similar to the SQL query language used for certain databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|    model|unit_price|\n",
      "+---------+----------+\n",
      "| iPhone 7|     739.0|\n",
      "| iPhone 6|     549.0|\n",
      "|iPhone 6s|     585.0|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phone_df.select(\"model\", \"unit_price\").where(\"brand='Apple'\").orderBy('stock', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative way of doing the same as the cell above is using `phone_df[\"brand\"]` in the where clause. This is longer to type but intuitively more clear and easier to read. There is no ambiguity for the Spark parser with this notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|    model|unit_price|\n",
      "+---------+----------+\n",
      "| iPhone 7|     739.0|\n",
      "| iPhone 6|     549.0|\n",
      "|iPhone 6s|     585.0|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phone_df.select(\"model\", \"unit_price\").where(phone_df[\"brand\"]==\"Apple\").orderBy('stock', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3\n",
    "Select all phones with a unit price larger than 300 and of which there are more than two in stock. Display the remaining phones, ordered by brand, followed by stock. Use whatever column specification syntax you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+-----+\n",
      "|  brand|    model|unit_price|stock|\n",
      "+-------+---------+----------+-----+\n",
      "|  Apple|iPhone 6s|     585.0|    5|\n",
      "|Samsung|Galaxy S6|     414.0|    5|\n",
      "|  Apple| iPhone 6|     549.0|    6|\n",
      "| Google|    Pixel|     859.0|    8|\n",
      "|Samsung|Galaxy S7|     539.0|   10|\n",
      "|  Apple| iPhone 7|     739.0|   11|\n",
      "+-------+---------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phone_df.select('brand', 'model', 'unit_price', 'stock').filter((phone_df.unit_price > 300) & (phone_df.stock > 2)).orderBy('brand').orderBy('stock').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating data\n",
    "An important part of data processing is the ability to combine multiple records, like we did with `reduceByKey`. In the DataFrame API this is a two-step process: grouping data, and then applying a function to the data.\n",
    "\n",
    "First we group the data using the [`groupBy`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy) method. `groupBy` can operate on one or multiple columns. It will not actually perform the grouping but create a reference to a [`GroupedData`](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.GroupedData) object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.group.GroupedData'>\n"
     ]
    }
   ],
   "source": [
    "grouped_df = phone_df.groupBy('brand')\n",
    "print(type(grouped_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data is grouped we can apply one of the standard aggregation functions on it. They are listed at the [`GroupedData`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData) API documentation. These are: `min`, `max`, `mean`, `sum` and `count`. We can apply an aggregation to all columns or to a subset of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>min(unit_price)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Samsung</td>\n",
       "      <td>297.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google</td>\n",
       "      <td>859.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple</td>\n",
       "      <td>549.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     brand  min(unit_price)\n",
       "0  Samsung            297.0\n",
       "1   Google            859.0\n",
       "2    Apple            549.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minimum for all columns\n",
    "min_df = grouped_df.min('unit_price')\n",
    "\n",
    "min_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `min(unit_price)` is the name of the new column. We can rename this column with the [`withColumnRenamed`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumnRenamed) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|  brand|min_unit_price|\n",
      "+-------+--------------+\n",
      "|Samsung|         297.0|\n",
      "| Google|         859.0|\n",
      "|  Apple|         549.0|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_df.withColumnRenamed('min(unit_price)', 'min_unit_price').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4\n",
    "\n",
    "Compute the maximum  of the unit_price per brand and rename the resulting column to `max`.\n",
    "(We assume you can do this in one line. Feel free to adapt the cell and use more lines if you want.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>max(unit_price)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Samsung</td>\n",
       "      <td>841.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google</td>\n",
       "      <td>959.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple</td>\n",
       "      <td>739.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     brand  max(unit_price)\n",
       "0  Samsung            841.0\n",
       "1   Google            959.0\n",
       "2    Apple            739.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "max_df = phone_df.groupBy('brand').max('unit_price')\n",
    "max_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate multiple aggregations over our data, we use the [`agg`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.agg) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------------+\n",
      "|  brand|sum(stock)|  avg(unit_price)|\n",
      "+-------+----------+-----------------+\n",
      "|Samsung|        22|           522.75|\n",
      "| Google|        10|            909.0|\n",
      "|  Apple|        22|624.3333333333334|\n",
      "+-------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take the sum of the stock column, and calculate the mean of the unit_price column, in one go\n",
    "sum_df = grouped_df.agg({'stock': 'sum', 'unit_price': 'mean'})\n",
    "\n",
    "sum_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL\n",
    "The SQL API aims to be ANSI-SQL SQL2003 and Hive-SQL compatible. The expressiveness is very similar to the DataFrame methods we just discussed. You can access the SQL API from the SparkSession by using `spark.sql`. Below is a query performed using Spark's DataFrame API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|    model|\n",
      "+---------+\n",
      "| iPhone 7|\n",
      "|    Pixel|\n",
      "|Galaxy S7|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame version\n",
    "res_df = phone_df.filter(phone_df['stock'] > 7).select('model')\n",
    "res_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SQL version of the query requires us to 'register' the DataFrame as an SQL table first: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|    model|\n",
      "+---------+\n",
      "| iPhone 7|\n",
      "|    Pixel|\n",
      "|Galaxy S7|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL version\n",
    "\n",
    "# Register the phone_df DataFrame within SQL as a table with name 'phones'\n",
    "phone_df.createOrReplaceTempView('phones')\n",
    "\n",
    "# Perform the SQL query on the 'phones' table\n",
    "res_df = spark.sql('SELECT model FROM phones WHERE stock > 7')\n",
    "res_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 5\n",
    "Transform the following query into SQL syntax, and run it using `spark.sql`. Show the resulting data frame.\n",
    "```\n",
    "phone_df.select(\"model\", \"unit_price\").where(phone_df[\"brand\"]==\"Apple\").orderBy('stock', ascending=False)\n",
    "```\n",
    "**Hint**: use the `phones` table in your SQL query, similar to the cell above.\n",
    "<br>\n",
    "**Hint**: for an SQL syntax cheat sheet, see this [pdf](http://files.zeroturnaround.com/pdf/zt_sql_cheat_sheet.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|    model|unit_price|\n",
      "+---------+----------+\n",
      "|iPhone 6s|     585.0|\n",
      "| iPhone 6|     549.0|\n",
      "| iPhone 7|     739.0|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_df = spark.sql('SELECT model, unit_price FROM phones WHERE brand = \"Apple\" ORDER BY stock')\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining with other data sets\n",
    "Often you want to combine multiple datasets on a shared column. In this example we create an extra table with information about phone manufacturers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-------------+-------------+\n",
      "|company_name| hq_country|founding_year|          ceo|\n",
      "+------------+-----------+-------------+-------------+\n",
      "|      Google|        USA|         1998|Sundar Pichai|\n",
      "|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
      "|       Apple|        USA|         1976|     Tim Cook|\n",
      "+------------+-----------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "companies = [\n",
    "    ('Google', 'USA', 1998, 'Sundar Pichai'),\n",
    "    ('Samsung', 'South Korea', 1938 ,'Oh-Hyun Kwon' ),\n",
    "    ('Apple', 'USA', 1976 ,'Tim Cook')\n",
    "]\n",
    "\n",
    "columns = ['company_name', 'hq_country', 'founding_year', 'ceo']\n",
    "\n",
    "company_df = spark.createDataFrame(companies, columns)\n",
    "company_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To join two DataFrames, we use the [`join`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join) method on one of the DataFrames. This method takes two arguments: (1) the other DataFrame and (2) a join relation, providing an expression on what columns to join on from both DataFrames. Here we join the two DataFrames on the brand/company_name columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+-----+----------+------------+-----------+-------------+-------------+\n",
      "|        model|  brand|stock|unit_price|company_name| hq_country|founding_year|          ceo|\n",
      "+-------------+-------+-----+----------+------------+-----------+-------------+-------------+\n",
      "|    Galaxy S7|Samsung|   10|     539.0|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
      "|    Galaxy S6|Samsung|    5|     414.0|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
      "|    Galaxy A5|Samsung|    7|     297.0|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
      "|Galaxy Note 7|Samsung|    0|     841.0|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
      "|        Pixel| Google|    8|     859.0|      Google|        USA|         1998|Sundar Pichai|\n",
      "|     Pixel XL| Google|    2|     959.0|      Google|        USA|         1998|Sundar Pichai|\n",
      "|     iPhone 6|  Apple|    6|     549.0|       Apple|        USA|         1976|     Tim Cook|\n",
      "|    iPhone 6s|  Apple|    5|     585.0|       Apple|        USA|         1976|     Tim Cook|\n",
      "|     iPhone 7|  Apple|   11|     739.0|       Apple|        USA|         1976|     Tim Cook|\n",
      "+-------------+-------+-----+----------+------------+-----------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df = phone_df.join(company_df, phone_df['brand'] == company_df['company_name'])\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 6\n",
    "Join the phone data frame on the company data frame. Select only the models for which the stock is greater than 7, and of which the company HQ is located in the USA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+----------+------------+----------+-------------+-------------+\n",
      "|   model| brand|stock|unit_price|company_name|hq_country|founding_year|          ceo|\n",
      "+--------+------+-----+----------+------------+----------+-------------+-------------+\n",
      "|   Pixel|Google|    8|     859.0|      Google|       USA|         1998|Sundar Pichai|\n",
      "|iPhone 7| Apple|   11|     739.0|       Apple|       USA|         1976|     Tim Cook|\n",
      "+--------+------+-----+----------+------------+----------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = phone_df.join(company_df, phone_df['brand'] == company_df['company_name']).filter((phone_df.stock > 7) & (company_df.hq_country == 'USA'))\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark's logical and physical plans\n",
    "Under the hood, Spark performs query optimization using its Catalyst optimizer. Catalyst will spot opportunities for simplification and speed-up, and generate an optimized _physical plan_ that is then executed by Spark.\n",
    "\n",
    "One of the optimizations Catalyst performs is _predicate pushdown_. It is easiest to show this mechanism by means of the example above. We can have Spark _explain_ to us what a logical and physical plan look like for a given query, using the [`explain`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.explain) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('model, None)]\n",
      "+- Filter (stock#11L > cast(7 as bigint))\n",
      "   +- Filter (hq_country#937 = USA)\n",
      "      +- Join Inner, (brand#10 = company_name#936)\n",
      "         :- LogicalRDD [model#9, brand#10, stock#11L, unit_price#12]\n",
      "         +- LogicalRDD [company_name#936, hq_country#937, founding_year#938L, ceo#939]\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "model: string\n",
      "Project [model#9]\n",
      "+- Filter (stock#11L > cast(7 as bigint))\n",
      "   +- Filter (hq_country#937 = USA)\n",
      "      +- Join Inner, (brand#10 = company_name#936)\n",
      "         :- LogicalRDD [model#9, brand#10, stock#11L, unit_price#12]\n",
      "         +- LogicalRDD [company_name#936, hq_country#937, founding_year#938L, ceo#939]\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [model#9]\n",
      "+- Join Inner, (brand#10 = company_name#936)\n",
      "   :- Project [model#9, brand#10]\n",
      "   :  +- Filter ((isnotnull(stock#11L) && (stock#11L > 7)) && isnotnull(brand#10))\n",
      "   :     +- LogicalRDD [model#9, brand#10, stock#11L, unit_price#12]\n",
      "   +- Project [company_name#936]\n",
      "      +- Filter ((isnotnull(hq_country#937) && (hq_country#937 = USA)) && isnotnull(company_name#936))\n",
      "         +- LogicalRDD [company_name#936, hq_country#937, founding_year#938L, ceo#939]\n",
      "\n",
      "== Physical Plan ==\n",
      "*Project [model#9]\n",
      "+- *SortMergeJoin [brand#10], [company_name#936], Inner\n",
      "   :- *Sort [brand#10 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(brand#10, 200)\n",
      "   :     +- *Project [model#9, brand#10]\n",
      "   :        +- *Filter ((isnotnull(stock#11L) && (stock#11L > 7)) && isnotnull(brand#10))\n",
      "   :           +- Scan ExistingRDD[model#9,brand#10,stock#11L,unit_price#12]\n",
      "   +- *Sort [company_name#936 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(company_name#936, 200)\n",
      "         +- *Project [company_name#936]\n",
      "            +- *Filter ((isnotnull(hq_country#937) && (hq_country#937 = USA)) && isnotnull(company_name#936))\n",
      "               +- Scan ExistingRDD[company_name#936,hq_country#937,founding_year#938L,ceo#939]\n"
     ]
    }
   ],
   "source": [
    "phone_df \\\n",
    "    .join(company_df, phone_df['brand'] == company_df['company_name']) \\\n",
    "    .filter(company_df['hq_country'] == 'USA') \\\n",
    "    .filter(phone_df['stock'] > 7) \\\n",
    "    .select('model') \\\n",
    "    .explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first block of text is on **the parsed logical plan**. Each plan is a tree of operations, where operations in the leafs of the tree (the right-most entries) are executed first. As you can see, the parsed logical plan will involve a join on two `LogicalRDD` objects (the company and phone data frames), after which there are two filter operations. In sequence, they filter the `hq_country` column, followed by the `stock` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After resolving references for the query strings, we end up with the **analyzed logical plan**. Please note that the `unresolvedalias('model` in the parsed logical plan is now resolved to its appropriate type (`string`). Also note that the tree of operations is not changed going from parsed to analyzed logical plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catalyst will now optimize the logical plan. In this case, Catalyst will notice that the two filter operations (`hq_country` on `company_df`) and (`stock` on `phone_df`) are performed _after joining_. However, since both operations only use columns from their own data frame, these filter operations can be performed **before** joining. In this way, the result of the filter operations are much smaller tables, and the resulting join will be much quicker to do as well. <br>\n",
    "Catalyst will move these filter operations down into the operations tree, such that we will filter first, and join later. This is known as _predicate pushdown_, and its effect can be seen in the **optimized logical plan**. The `Join Inner` on the second line of the plan will now contain two subtrees, one filtering the `company_df` data frame, the other filtering the `phone_df` data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicate pushdown is just one of the optimizations Catalyst can perform. Also, if the execution of a query takes an inordinate amount of time, it may help to use the [`explain`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.explain) method to see its logical and physical plans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 7\n",
    "\n",
    "The problem below was taken from Coursera's MOOC [Big Data Analysis with Scala and Spark](https://www.coursera.org/learn/scala-spark-big-data) by the cole Polytechnique Fdrale de Lausanne. We adapted the problem for PySpark, and extended it.\n",
    "\n",
    "Let's assume we have a dataset with posts from a discussion forum. The entries of the dataset consist of an `author_id`, a `subforum_id`, the number of likes and a date. The data frame is constructed in the following cell.\n",
    "\n",
    "**We would like to know how many likes each author posted on each subforum. The table should show per `subforum_id` how many likes each author has, the highest number of likes first.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----+------+\n",
      "|author_id|subforum_id|likes|  date|\n",
      "+---------+-----------+-----+------+\n",
      "|        4|          1|    5|sept 5|\n",
      "|        1|          2|    3|sept 4|\n",
      "|        2|          2|   35|sept 3|\n",
      "|        3|          1|    1|sept 5|\n",
      "|        4|          1|   14|sept 5|\n",
      "|        3|          2|   12|sept 3|\n",
      "|        3|          1|   14|sept 5|\n",
      "|        3|          1|   10|sept 5|\n",
      "|        2|          2|   21|sept 5|\n",
      "+---------+-----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "posts_df = spark.createDataFrame(\n",
    "    [\n",
    "        (4, 1, 5, 'sept 5'),\n",
    "        (1, 2, 3, 'sept 4'),\n",
    "        (2, 2, 35, 'sept 3'),\n",
    "        (3, 1, 1, 'sept 5'),\n",
    "        (4, 1, 14, 'sept 5'),\n",
    "        (3, 2, 12, 'sept 3'),\n",
    "        (3, 1, 14, 'sept 5'),\n",
    "        (3, 1, 10, 'sept 5'),\n",
    "        (2, 2, 21, 'sept 5')\n",
    "    ],\n",
    "    ['author_id', 'subforum_id', 'likes', 'date']\n",
    ")\n",
    "posts_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please use a [`groupBy`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy), the [`sum`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.sum) aggregation function and an [`orderBy`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderBy) to come up with the desired dataFrame. Note that you want to order the results descendingly.\n",
    "Also note that you can use `groupBy` and `orderBy` on more than one column.\n",
    "\n",
    "If you get confused, break the problem into steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----------+\n",
      "|author_id|subforum_id|sum(likes)|\n",
      "+---------+-----------+----------+\n",
      "|        2|          2|        56|\n",
      "|        3|          1|        25|\n",
      "|        4|          1|        19|\n",
      "|        3|          2|        12|\n",
      "|        1|          2|         3|\n",
      "+---------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "posts_df.groupBy('author_id', 'subforum_id').agg({'likes': 'sum'}).orderBy('sum(likes)', ascending=False).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 8\n",
    "We also have two data frames containing author names and subforum names. Join these two tables on the DataFrame and repeat the query, now showing the author's name and forum's name instead of their IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, 'Brian Watt'),\n",
    "        (2, 'Esmeralda Andrews'),\n",
    "        (3, 'Mabel Frank'),\n",
    "        (4, 'Jan Lyndon'),\n",
    "        (5, 'Dione Thorpe')\n",
    "    ],\n",
    "    ['author_id', 'author_name']\n",
    ")\n",
    "\n",
    "subforums_df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, 'java'),\n",
    "        (2, 'python')\n",
    "    ],\n",
    "    ['subforum_id', 'subforum_name']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+----------+\n",
      "|subforum_name|      author_name|sum(likes)|\n",
      "+-------------+-----------------+----------+\n",
      "|       python|       Brian Watt|         3|\n",
      "|       python|      Mabel Frank|        12|\n",
      "|         java|       Jan Lyndon|        19|\n",
      "|         java|      Mabel Frank|        25|\n",
      "|       python|Esmeralda Andrews|        56|\n",
      "+-------------+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "posts_df.join(subforums_df, 'subforum_id').join(authors_df, 'author_id').groupBy('subforum_name', 'author_name').sum('likes').orderBy('sum(likes)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-dimensional aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with multi-dimensional data sets, we often may be interested in aggregrating across multiple dimensions of our data. One of the simplest operations to do so is [`pivot`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.pivot). Continuing with our forum posts data sets, we can _pivot_ a `GroupedData` object on another column. Please have look at the following example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+------+------+\n",
      "|subforum_name|sept 3|sept 4|sept 5|\n",
      "+-------------+------+------+------+\n",
      "|         java|  null|  null|    44|\n",
      "|       python|    47|     3|    21|\n",
      "+-------------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join all relevant data frames\n",
    "\n",
    "posts_joined_df = posts_df.join(subforums_df, 'subforum_id').join(authors_df, 'author_id')\n",
    "\n",
    "# Group by forum name, and pivot on the date, then sum the number of likes\n",
    "\n",
    "posts_joined_df \\\n",
    "    .groupBy('subforum_name') \\\n",
    "    .pivot('date') \\\n",
    "    .sum('likes') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to look at our data across more than two dimensions, we will need to look at a number of different methods. The following cell calculates the subtotals and grand totals of the number of likes for each subforum and author using the [`rollup`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.rollup) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+----------+\n",
      "|subforum_name|      author_name|sum(likes)|\n",
      "+-------------+-----------------+----------+\n",
      "|         null|             null|       115|\n",
      "|         java|             null|        44|\n",
      "|         java|       Jan Lyndon|        19|\n",
      "|         java|      Mabel Frank|        25|\n",
      "|       python|             null|        71|\n",
      "|       python|       Brian Watt|         3|\n",
      "|       python|Esmeralda Andrews|        56|\n",
      "|       python|      Mabel Frank|        12|\n",
      "+-------------+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Roll up over subforum name and author name\n",
    "\n",
    "posts_joined_df \\\n",
    "    .rollup('subforum_name', 'author_name') \\\n",
    "    .sum('likes') \\\n",
    "    .orderBy('subforum_name', 'author_name') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the grand total number of likes across all forums is 115, where the Java subforum has 44 and the Python subforum has 71. Per subforum we also have a subtotal of the number of likes per person.\n",
    "\n",
    "As you can see, `rollup` takes into account an **hierarchy**. It will output:\n",
    "\n",
    "* a grand total (`null`, `null`);\n",
    "* a total for each forum (`<subforum_name>`, `<null>`);\n",
    "* a subtotal for each author and subforum (`<subforum_name>`, `<author_name>`)\n",
    "\n",
    "However, because of the same hierarchy it will not output a total for each author across all forums.\n",
    "\n",
    "In contrast, the [cube](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.cube) method will aggregate over _all possible combinations_ of dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+----------+\n",
      "|subforum_name|      author_name|sum(likes)|\n",
      "+-------------+-----------------+----------+\n",
      "|         null|             null|       115|\n",
      "|         null|       Brian Watt|         3|\n",
      "|         null|Esmeralda Andrews|        56|\n",
      "|         null|       Jan Lyndon|        19|\n",
      "|         null|      Mabel Frank|        37|\n",
      "|         java|             null|        44|\n",
      "|         java|       Jan Lyndon|        19|\n",
      "|         java|      Mabel Frank|        25|\n",
      "|       python|             null|        71|\n",
      "|       python|       Brian Watt|         3|\n",
      "|       python|Esmeralda Andrews|        56|\n",
      "|       python|      Mabel Frank|        12|\n",
      "+-------------+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "posts_joined_df \\\n",
    "    .cube('subforum_name', 'author_name') \\\n",
    "    .sum('likes') \\\n",
    "    .orderBy('subforum_name', 'author_name') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, `cube` calculates the aggregate for each possible combination of subforum name and author name. As with `rollup`, we still have our grand total and totals per subforum. In addition, though, we also have a subtotal for each author across all forums (`null`, `<author_name>`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading structured files/sources\n",
    "One of the advantages of using `DataFrames` is the ability to read already structured data and automatically import the structure in Spark. Spark contains readers for a number of formats such as csv, json, parquet, orc, text and jdbc. There are also third-party readers/connectors for databases such as MongoDB and Cassandra.\n",
    "\n",
    "Here we read some json-formatted tweets. The nested json structure is inferred, as you can see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contributors: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- entities: struct (nullable = true)\n",
      " |    |-- hashtags: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- text: string (nullable = true)\n",
      " |    |-- media: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- media_url: string (nullable = true)\n",
      " |    |    |    |-- media_url_https: string (nullable = true)\n",
      " |    |    |    |-- sizes: struct (nullable = true)\n",
      " |    |    |    |    |-- large: struct (nullable = true)\n",
      " |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |-- medium: struct (nullable = true)\n",
      " |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |-- small: struct (nullable = true)\n",
      " |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |-- thumb: struct (nullable = true)\n",
      " |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |-- source_status_id: long (nullable = true)\n",
      " |    |    |    |-- source_status_id_str: string (nullable = true)\n",
      " |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |    |-- symbols: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- trends: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- urls: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |    |-- user_mentions: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- screen_name: string (nullable = true)\n",
      " |-- extended_entities: struct (nullable = true)\n",
      " |    |-- media: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- media_url: string (nullable = true)\n",
      " |    |    |    |-- media_url_https: string (nullable = true)\n",
      " |    |    |    |-- sizes: struct (nullable = true)\n",
      " |    |    |    |    |-- large: struct (nullable = true)\n",
      " |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |-- medium: struct (nullable = true)\n",
      " |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |-- small: struct (nullable = true)\n",
      " |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |-- thumb: struct (nullable = true)\n",
      " |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |-- source_status_id: long (nullable = true)\n",
      " |    |    |    |-- source_status_id_str: string (nullable = true)\n",
      " |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |-- favorite_count: long (nullable = true)\n",
      " |-- favorited: boolean (nullable = true)\n",
      " |-- filter_level: string (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- id_str: string (nullable = true)\n",
      " |-- in_reply_to_screen_name: string (nullable = true)\n",
      " |-- in_reply_to_status_id: long (nullable = true)\n",
      " |-- in_reply_to_status_id_str: string (nullable = true)\n",
      " |-- in_reply_to_user_id: long (nullable = true)\n",
      " |-- in_reply_to_user_id_str: string (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- place: struct (nullable = true)\n",
      " |    |-- bounding_box: struct (nullable = true)\n",
      " |    |    |-- coordinates: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- country_code: string (nullable = true)\n",
      " |    |-- full_name: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- place_type: string (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |-- possibly_sensitive: boolean (nullable = true)\n",
      " |-- retweet_count: long (nullable = true)\n",
      " |-- retweeted: boolean (nullable = true)\n",
      " |-- retweeted_status: struct (nullable = true)\n",
      " |    |-- contributors: string (nullable = true)\n",
      " |    |-- coordinates: struct (nullable = true)\n",
      " |    |    |-- coordinates: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- entities: struct (nullable = true)\n",
      " |    |    |-- hashtags: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |-- text: string (nullable = true)\n",
      " |    |    |-- media: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |-- media_url: string (nullable = true)\n",
      " |    |    |    |    |-- media_url_https: string (nullable = true)\n",
      " |    |    |    |    |-- sizes: struct (nullable = true)\n",
      " |    |    |    |    |    |-- large: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |    |-- medium: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |    |-- small: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |    |-- thumb: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |-- source_status_id: long (nullable = true)\n",
      " |    |    |    |    |-- source_status_id_str: string (nullable = true)\n",
      " |    |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- symbols: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- trends: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- urls: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- user_mentions: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |-- screen_name: string (nullable = true)\n",
      " |    |-- extended_entities: struct (nullable = true)\n",
      " |    |    |-- media: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |-- media_url: string (nullable = true)\n",
      " |    |    |    |    |-- media_url_https: string (nullable = true)\n",
      " |    |    |    |    |-- sizes: struct (nullable = true)\n",
      " |    |    |    |    |    |-- large: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |    |-- medium: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |    |-- small: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |    |-- thumb: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |-- source_status_id: long (nullable = true)\n",
      " |    |    |    |    |-- source_status_id_str: string (nullable = true)\n",
      " |    |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |-- favorite_count: long (nullable = true)\n",
      " |    |-- favorited: boolean (nullable = true)\n",
      " |    |-- filter_level: string (nullable = true)\n",
      " |    |-- geo: struct (nullable = true)\n",
      " |    |    |-- coordinates: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- id_str: string (nullable = true)\n",
      " |    |-- in_reply_to_screen_name: string (nullable = true)\n",
      " |    |-- in_reply_to_status_id: long (nullable = true)\n",
      " |    |-- in_reply_to_status_id_str: string (nullable = true)\n",
      " |    |-- in_reply_to_user_id: long (nullable = true)\n",
      " |    |-- in_reply_to_user_id_str: string (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- place: struct (nullable = true)\n",
      " |    |    |-- bounding_box: struct (nullable = true)\n",
      " |    |    |    |-- coordinates: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- country: string (nullable = true)\n",
      " |    |    |-- country_code: string (nullable = true)\n",
      " |    |    |-- full_name: string (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- place_type: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |-- possibly_sensitive: boolean (nullable = true)\n",
      " |    |-- retweet_count: long (nullable = true)\n",
      " |    |-- retweeted: boolean (nullable = true)\n",
      " |    |-- scopes: struct (nullable = true)\n",
      " |    |    |-- followers: boolean (nullable = true)\n",
      " |    |-- source: string (nullable = true)\n",
      " |    |-- text: string (nullable = true)\n",
      " |    |-- truncated: boolean (nullable = true)\n",
      " |    |-- user: struct (nullable = true)\n",
      " |    |    |-- contributors_enabled: boolean (nullable = true)\n",
      " |    |    |-- created_at: string (nullable = true)\n",
      " |    |    |-- default_profile: boolean (nullable = true)\n",
      " |    |    |-- default_profile_image: boolean (nullable = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- favourites_count: long (nullable = true)\n",
      " |    |    |-- follow_request_sent: string (nullable = true)\n",
      " |    |    |-- followers_count: long (nullable = true)\n",
      " |    |    |-- following: string (nullable = true)\n",
      " |    |    |-- friends_count: long (nullable = true)\n",
      " |    |    |-- geo_enabled: boolean (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |-- is_translator: boolean (nullable = true)\n",
      " |    |    |-- lang: string (nullable = true)\n",
      " |    |    |-- listed_count: long (nullable = true)\n",
      " |    |    |-- location: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- notifications: string (nullable = true)\n",
      " |    |    |-- profile_background_color: string (nullable = true)\n",
      " |    |    |-- profile_background_image_url: string (nullable = true)\n",
      " |    |    |-- profile_background_image_url_https: string (nullable = true)\n",
      " |    |    |-- profile_background_tile: boolean (nullable = true)\n",
      " |    |    |-- profile_banner_url: string (nullable = true)\n",
      " |    |    |-- profile_image_url: string (nullable = true)\n",
      " |    |    |-- profile_image_url_https: string (nullable = true)\n",
      " |    |    |-- profile_link_color: string (nullable = true)\n",
      " |    |    |-- profile_sidebar_border_color: string (nullable = true)\n",
      " |    |    |-- profile_sidebar_fill_color: string (nullable = true)\n",
      " |    |    |-- profile_text_color: string (nullable = true)\n",
      " |    |    |-- profile_use_background_image: boolean (nullable = true)\n",
      " |    |    |-- protected: boolean (nullable = true)\n",
      " |    |    |-- screen_name: string (nullable = true)\n",
      " |    |    |-- statuses_count: long (nullable = true)\n",
      " |    |    |-- time_zone: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- utc_offset: long (nullable = true)\n",
      " |    |    |-- verified: boolean (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- timestamp_ms: string (nullable = true)\n",
      " |-- truncated: boolean (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- contributors_enabled: boolean (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- default_profile: boolean (nullable = true)\n",
      " |    |-- default_profile_image: boolean (nullable = true)\n",
      " |    |-- description: string (nullable = true)\n",
      " |    |-- favourites_count: long (nullable = true)\n",
      " |    |-- follow_request_sent: string (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- following: string (nullable = true)\n",
      " |    |-- friends_count: long (nullable = true)\n",
      " |    |-- geo_enabled: boolean (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- id_str: string (nullable = true)\n",
      " |    |-- is_translator: boolean (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- listed_count: long (nullable = true)\n",
      " |    |-- location: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- notifications: string (nullable = true)\n",
      " |    |-- profile_background_color: string (nullable = true)\n",
      " |    |-- profile_background_image_url: string (nullable = true)\n",
      " |    |-- profile_background_image_url_https: string (nullable = true)\n",
      " |    |-- profile_background_tile: boolean (nullable = true)\n",
      " |    |-- profile_banner_url: string (nullable = true)\n",
      " |    |-- profile_image_url: string (nullable = true)\n",
      " |    |-- profile_image_url_https: string (nullable = true)\n",
      " |    |-- profile_link_color: string (nullable = true)\n",
      " |    |-- profile_sidebar_border_color: string (nullable = true)\n",
      " |    |-- profile_sidebar_fill_color: string (nullable = true)\n",
      " |    |-- profile_text_color: string (nullable = true)\n",
      " |    |-- profile_use_background_image: boolean (nullable = true)\n",
      " |    |-- protected: boolean (nullable = true)\n",
      " |    |-- screen_name: string (nullable = true)\n",
      " |    |-- statuses_count: long (nullable = true)\n",
      " |    |-- time_zone: string (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |    |-- utc_offset: long (nullable = true)\n",
      " |    |-- verified: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweet_df = spark.read.format(\"json\").load('../data/tweets.json')\n",
    "tweet_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This structure is squeezed into a table. When we convert to Pandas we can see what the first tweet looks like in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contributors</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>entities</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>filter_level</th>\n",
       "      <th>geo</th>\n",
       "      <th>id</th>\n",
       "      <th>...</th>\n",
       "      <th>place</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>retweeted_status</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp_ms</th>\n",
       "      <th>truncated</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Wed Apr 29 13:26:48 +0000 2015</td>\n",
       "      <td>([], None, [], [], [], [(48305190, 48305190, [...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>None</td>\n",
       "      <td>593406077439516672</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>@OdekedeJong Omdat ik het zelf ook ervaar en m...</td>\n",
       "      <td>1430314008470</td>\n",
       "      <td>False</td>\n",
       "      <td>(False, Thu Mar 04 11:16:36 +0000 2010, False,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  contributors coordinates                      created_at  \\\n",
       "0         None        None  Wed Apr 29 13:26:48 +0000 2015   \n",
       "\n",
       "                                            entities extended_entities  \\\n",
       "0  ([], None, [], [], [], [(48305190, 48305190, [...              None   \n",
       "\n",
       "   favorite_count  favorited filter_level   geo                  id  \\\n",
       "0               0      False          low  None  593406077439516672   \n",
       "\n",
       "                         ...                         place possibly_sensitive  \\\n",
       "0                        ...                          None              False   \n",
       "\n",
       "   retweet_count retweeted  retweeted_status  \\\n",
       "0              0     False              None   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...   \n",
       "\n",
       "                                                text   timestamp_ms  \\\n",
       "0  @OdekedeJong Omdat ik het zelf ook ervaar en m...  1430314008470   \n",
       "\n",
       "   truncated                                               user  \n",
       "0      False  (False, Thu Mar 04 11:16:36 +0000 2010, False,...  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.toPandas().head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the elements of a tweet are flattened to fit into the row of a table. We can also convert a tweet to a Python dictionary to see its nested structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contributors': None,\n",
       " 'coordinates': None,\n",
       " 'created_at': 'Wed Apr 29 13:26:48 +0000 2015',\n",
       " 'entities': {'hashtags': [],\n",
       "  'media': None,\n",
       "  'symbols': [],\n",
       "  'trends': [],\n",
       "  'urls': [],\n",
       "  'user_mentions': [{'id': 48305190,\n",
       "    'id_str': '48305190',\n",
       "    'indices': [0, 12],\n",
       "    'name': 'Odeke de Jong',\n",
       "    'screen_name': 'OdekedeJong'}]},\n",
       " 'extended_entities': None,\n",
       " 'favorite_count': 0,\n",
       " 'favorited': False,\n",
       " 'filter_level': 'low',\n",
       " 'geo': None,\n",
       " 'id': 593406077439516672,\n",
       " 'id_str': '593406077439516672',\n",
       " 'in_reply_to_screen_name': 'OdekedeJong',\n",
       " 'in_reply_to_status_id': 593403994481020928,\n",
       " 'in_reply_to_status_id_str': '593403994481020928',\n",
       " 'in_reply_to_user_id': 48305190,\n",
       " 'in_reply_to_user_id_str': '48305190',\n",
       " 'lang': 'nl',\n",
       " 'place': None,\n",
       " 'possibly_sensitive': False,\n",
       " 'retweet_count': 0,\n",
       " 'retweeted': False,\n",
       " 'retweeted_status': None,\n",
       " 'source': '<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web Client</a>',\n",
       " 'text': '@OdekedeJong Omdat ik het zelf ook ervaar en mijn omgeving ook wel. Er hangt soms een bepaalde sfeer waardoor je niet alles kan/durft.',\n",
       " 'timestamp_ms': '1430314008470',\n",
       " 'truncated': False,\n",
       " 'user': {'contributors_enabled': False,\n",
       "  'created_at': 'Thu Mar 04 11:16:36 +0000 2010',\n",
       "  'default_profile': False,\n",
       "  'default_profile_image': False,\n",
       "  'description': '* Receptor Agent * Bachelor of  Journalism * Nieuwsjunk * Film- en seriefan * Post- en zonliefhebber *',\n",
       "  'favourites_count': 50,\n",
       "  'follow_request_sent': None,\n",
       "  'followers_count': 138,\n",
       "  'following': None,\n",
       "  'friends_count': 222,\n",
       "  'geo_enabled': True,\n",
       "  'id': 119697699,\n",
       "  'id_str': '119697699',\n",
       "  'is_translator': False,\n",
       "  'lang': 'en',\n",
       "  'listed_count': 2,\n",
       "  'location': '',\n",
       "  'name': 'Claudia',\n",
       "  'notifications': None,\n",
       "  'profile_background_color': '8B542B',\n",
       "  'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme8/bg.gif',\n",
       "  'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme8/bg.gif',\n",
       "  'profile_background_tile': False,\n",
       "  'profile_banner_url': 'https://pbs.twimg.com/profile_banners/119697699/1357216053',\n",
       "  'profile_image_url': 'http://pbs.twimg.com/profile_images/474540728983355392/dWRb-CDv_normal.jpeg',\n",
       "  'profile_image_url_https': 'https://pbs.twimg.com/profile_images/474540728983355392/dWRb-CDv_normal.jpeg',\n",
       "  'profile_link_color': '9D582E',\n",
       "  'profile_sidebar_border_color': 'D9B17E',\n",
       "  'profile_sidebar_fill_color': 'EADEAA',\n",
       "  'profile_text_color': '333333',\n",
       "  'profile_use_background_image': True,\n",
       "  'protected': False,\n",
       "  'screen_name': 'Claudia_NL',\n",
       "  'statuses_count': 3993,\n",
       "  'time_zone': 'Athens',\n",
       "  'url': None,\n",
       "  'utc_offset': 10800,\n",
       "  'verified': False}}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.head(198)[0].asDict(recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 9\n",
    "Select the screen name and language of the user, as well as the text field.\n",
    "\n",
    "**Hint**: nested fields can be selected using the dot notation, i.e. `df.select('<parent>.<child>')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Claudia_NL</td>\n",
       "      <td>en</td>\n",
       "      <td>@OdekedeJong Omdat ik het zelf ook ervaar en m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MiesjeB</td>\n",
       "      <td>nl</td>\n",
       "      <td>RT @RHoogland: The game is on, vrienden. Vrijd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>martine_vandijk</td>\n",
       "      <td>nl</td>\n",
       "      <td>@deBeschaving Snap ik! Ben nou eenmaal wat ong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tessaaatje</td>\n",
       "      <td>en</td>\n",
       "      <td>Jeminee ik word nu pas wakker wat is dit. Zo l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>juradoscrime</td>\n",
       "      <td>nl</td>\n",
       "      <td>@mayravdzwaag hij was helemaal niet leuk en ik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dewestkrant</td>\n",
       "      <td>nl</td>\n",
       "      <td>Het lijken wel kogelgaten, maar volgens de pol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PronkRijpstra</td>\n",
       "      <td>nl</td>\n",
       "      <td>@bvpuntcom ja hoor! Deze is van net  http://...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PostNL</td>\n",
       "      <td>nl</td>\n",
       "      <td>@mamarije30 Dat ga ik even voor je kijken, heb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hildafeenstra</td>\n",
       "      <td>nl</td>\n",
       "      <td>Na ruim 1,5 jaar vandaag mijn laatste werkdag ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GewoonBasten</td>\n",
       "      <td>nl</td>\n",
       "      <td>@RowfeyVFX @Jerry_Kuijper heb ik ook gezegt! i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MaassluisInfo</td>\n",
       "      <td>nl</td>\n",
       "      <td>Waternetsteiger tijdelijk uit de vaart\\n\\nWoen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>manonismanon</td>\n",
       "      <td>nl</td>\n",
       "      <td>@Cindytekent haha niet erg, ik voelde wat je b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>EricSweens</td>\n",
       "      <td>en</td>\n",
       "      <td>Ben jij de nieuwe Service Cordinator Elektrot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>werkenamerpoort</td>\n",
       "      <td>en</td>\n",
       "      <td>@Astad_EU @WeekMert @SWValkenswaard en zo niet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LeendertTerlouw</td>\n",
       "      <td>nl</td>\n",
       "      <td>RT @deBezieling: Wonderlijk: het meest nederig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        screen_name lang                                               text\n",
       "0        Claudia_NL   en  @OdekedeJong Omdat ik het zelf ook ervaar en m...\n",
       "1           MiesjeB   nl  RT @RHoogland: The game is on, vrienden. Vrijd...\n",
       "2   martine_vandijk   nl  @deBeschaving Snap ik! Ben nou eenmaal wat ong...\n",
       "3        Tessaaatje   en  Jeminee ik word nu pas wakker wat is dit. Zo l...\n",
       "4      juradoscrime   nl  @mayravdzwaag hij was helemaal niet leuk en ik...\n",
       "5       dewestkrant   nl  Het lijken wel kogelgaten, maar volgens de pol...\n",
       "6     PronkRijpstra   nl  @bvpuntcom ja hoor! Deze is van net  http://...\n",
       "7            PostNL   nl  @mamarije30 Dat ga ik even voor je kijken, heb...\n",
       "8     hildafeenstra   nl  Na ruim 1,5 jaar vandaag mijn laatste werkdag ...\n",
       "9      GewoonBasten   nl  @RowfeyVFX @Jerry_Kuijper heb ik ook gezegt! i...\n",
       "10    MaassluisInfo   nl  Waternetsteiger tijdelijk uit de vaart\\n\\nWoen...\n",
       "11     manonismanon   nl  @Cindytekent haha niet erg, ik voelde wat je b...\n",
       "12       EricSweens   en  Ben jij de nieuwe Service Cordinator Elektrot...\n",
       "13  werkenamerpoort   en  @Astad_EU @WeekMert @SWValkenswaard en zo niet...\n",
       "14  LeendertTerlouw   nl  RT @deBezieling: Wonderlijk: het meest nederig..."
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_df = tweet_df.select('user.screen_name', 'user.lang', 'text')\n",
    "name_df.toPandas().head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 10\n",
    "Count the number of tweets per user, and display the top 10 most-tweeting users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n",
      "|           name|sum(num_tweets)|\n",
      "+---------------+---------------+\n",
      "|     news24hnld|        6541613|\n",
      "|            KLM|        2235614|\n",
      "|      NS_online|        1438124|\n",
      "|   loveinground|         813134|\n",
      "|tmobile_webcare|         499221|\n",
      "|    atomsoffice|         417921|\n",
      "|      NLvandaag|         395668|\n",
      "|   nieuws_media|         312414|\n",
      "|    mannenNETbe|         300335|\n",
      "|     KPNwebcare|         265483|\n",
      "+---------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "status_df = tweet_df.select(expr('user.screen_name as name'), expr('user.statuses_count as num_tweets')).groupBy('name').sum('num_tweets').orderBy('sum(num_tweets)', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word count in DataFrames\n",
    "\n",
    "It is also possible to use DataFrames for less-structured data such as text. Here we show how you could do a word count with DataFrames.\n",
    "\n",
    "The following chained query contains a number of methods you haven't seen before, and we'll go through it line by line in the subsequent cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|word| count|\n",
      "+----+------+\n",
      "|    |198753|\n",
      "| the| 23288|\n",
      "|   I| 22225|\n",
      "| and| 18653|\n",
      "|  to| 16373|\n",
      "|  of| 15725|\n",
      "|   a| 12796|\n",
      "| you| 12186|\n",
      "|  my| 10839|\n",
      "|  in| 10016|\n",
      "|   d|  8954|\n",
      "|  is|  8414|\n",
      "|that|  8343|\n",
      "| not|  8038|\n",
      "|  me|  7752|\n",
      "|   s|  7487|\n",
      "| And|  7457|\n",
      "|with|  6802|\n",
      "|  it|  6760|\n",
      "|  be|  6412|\n",
      "+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "spark \\\n",
    "    .read.text('../data/shakespeare.txt') \\\n",
    "    .select(explode(split(\"value\", \"\\W+\")).alias(\"word\")) \\\n",
    "    .groupBy(\"word\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what happens here, we break it down into steps. First we read in the data file and inspect the DataFrame. It contains one column, called `value` by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|This is the 100th...|\n",
      "|is presented in c...|\n",
      "|Library of the Fu...|\n",
      "|often releases Et...|\n",
      "|                    |\n",
      "|         Shakespeare|\n",
      "|                    |\n",
      "|*This Etext has c...|\n",
      "|                    |\n",
      "|<<THIS ELECTRONIC...|\n",
      "|SHAKESPEARE IS CO...|\n",
      "|PROVIDED BY PROJE...|\n",
      "|WITH PERMISSION. ...|\n",
      "|DISTRIBUTED SO LO...|\n",
      "|PERSONAL USE ONLY...|\n",
      "|COMMERCIALLY.  PR...|\n",
      "|SERVICE THAT CHAR...|\n",
      "|                    |\n",
      "|*Project Gutenber...|\n",
      "|in the presentati...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swan_df = spark.read.text('../data/shakespeare.txt')\n",
    "swan_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column name `value` explains why it is mentioned inside the [`split`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.split) function. Let's call the `select` method but omit `explode` and see what happens. Please note that with [`alias`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.alias) we rename the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                word|\n",
      "+--------------------+\n",
      "|[This, is, the, 1...|\n",
      "|[is, presented, i...|\n",
      "|[Library, of, the...|\n",
      "|[often, releases,...|\n",
      "|                  []|\n",
      "|       [Shakespeare]|\n",
      "|                  []|\n",
      "|[, This, Etext, h...|\n",
      "|                  []|\n",
      "|[, THIS, ELECTRON...|\n",
      "|[SHAKESPEARE, IS,...|\n",
      "|[PROVIDED, BY, PR...|\n",
      "|[WITH, PERMISSION...|\n",
      "|[DISTRIBUTED, SO,...|\n",
      "|[PERSONAL, USE, O...|\n",
      "|[COMMERCIALLY, PR...|\n",
      "|[SERVICE, THAT, C...|\n",
      "|                  []|\n",
      "|[, Project, Guten...|\n",
      "|[in, the, present...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_df = swan_df.select(split(\"value\", \"\\W+\").alias(\"word\"))\n",
    "split_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the schema, we can see that `word` is actually an array of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we would like to have a row for each word, which is where [`explode`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.explode) comes in. `explode` will return a new row for each element in the `value` list, containing all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|       word|\n",
      "+-----------+\n",
      "|       This|\n",
      "|         is|\n",
      "|        the|\n",
      "|      100th|\n",
      "|      Etext|\n",
      "|       file|\n",
      "|  presented|\n",
      "|         by|\n",
      "|    Project|\n",
      "|  Gutenberg|\n",
      "|        and|\n",
      "|         is|\n",
      "|  presented|\n",
      "|         in|\n",
      "|cooperation|\n",
      "|       with|\n",
      "|      World|\n",
      "|    Library|\n",
      "|        Inc|\n",
      "|       from|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swan_df.select(explode(split(\"value\", \"\\W+\")).alias(\"word\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-defined functions\n",
    "\n",
    "In the previous example we used the built-in `split` function. It is also possible to define and use a custom user-defined function, or UDF. We'll show an example for the phone stock DataFrame first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+-----+----------+-----------+\n",
      "|        model|  brand|stock|unit_price|       cost|\n",
      "+-------------+-------+-----+----------+-----------+\n",
      "|     iPhone 6|  Apple|    6|     549.0|  Expensive|\n",
      "|    iPhone 6s|  Apple|    5|     585.0|  Expensive|\n",
      "|     iPhone 7|  Apple|   11|     739.0|  Expensive|\n",
      "|        Pixel| Google|    8|     859.0|  Expensive|\n",
      "|     Pixel XL| Google|    2|     959.0|  Expensive|\n",
      "|    Galaxy S7|Samsung|   10|     539.0|  Expensive|\n",
      "|    Galaxy S6|Samsung|    5|     414.0|Inexpensive|\n",
      "|    Galaxy A5|Samsung|    7|     297.0|Inexpensive|\n",
      "|Galaxy Note 7|Samsung|    0|     841.0|  Expensive|\n",
      "+-------------+-------+-----+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "exp_udf = udf(lambda price: \"Expensive\" if price >= 500 else \"Inexpensive\", StringType())\n",
    "\n",
    "phone_df.withColumn(\"cost\", exp_udf(phone_df['unit_price'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this manner, we can apply specialized function, like tokenizers, on DataFrames. However, we first must register them as UDFs and cannot simply pass them as (lambda) functions.\n",
    "\n",
    "Below we define a very simple tokenizer, just as an example. It uses Python's string `split`, and also lowers the case of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|This is the 100th...|\n",
      "|is presented in c...|\n",
      "|Library of the Fu...|\n",
      "|often releases Et...|\n",
      "|                    |\n",
      "|         Shakespeare|\n",
      "|                    |\n",
      "|*This Etext has c...|\n",
      "|                    |\n",
      "|<<THIS ELECTRONIC...|\n",
      "|SHAKESPEARE IS CO...|\n",
      "|PROVIDED BY PROJE...|\n",
      "|WITH PERMISSION. ...|\n",
      "|DISTRIBUTED SO LO...|\n",
      "|PERSONAL USE ONLY...|\n",
      "|COMMERCIALLY.  PR...|\n",
      "|SERVICE THAT CHAR...|\n",
      "|                    |\n",
      "|*Project Gutenber...|\n",
      "|in the presentati...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "def my_tokenize(s):\n",
    "    s = s.lower()\n",
    "    words = s.split()\n",
    "    return words\n",
    "\n",
    "returnType = ArrayType(StringType())\n",
    "\n",
    "tokenize_udf = udf(my_tokenize, returnType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 11\n",
    "Use the `tokenize_udf` function from the last cell to count words on the Shakespeare DataFrame `swan_df` instead of usng the `split` function. Display the top 10 most occurring words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the|27549|\n",
      "| and|26037|\n",
      "|   i|19540|\n",
      "|  to|18700|\n",
      "|  of|18010|\n",
      "|   a|14383|\n",
      "|  my|12455|\n",
      "|  in|10671|\n",
      "| you|10630|\n",
      "|that|10487|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_df = swan_df.select(explode(tokenize_udf('value')).alias('word'))\n",
    "word_df.groupBy('word').count().orderBy('count', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus exercise\n",
    "This exercise is optional. Perform a word count on the tweets' hashtags and show the top 10 most-used hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|    hashtag|count|\n",
      "+-----------+-----+\n",
      "|   pgbdebat|   10|\n",
      "|   pgbalarm|   10|\n",
      "|partnerruil|    4|\n",
      "|  freelance|    4|\n",
      "|        zzp|    4|\n",
      "|   vacature|    3|\n",
      "|maagdenhuis|    3|\n",
      "|        ZZP|    3|\n",
      "|        pgb|    2|\n",
      "|         NS|    2|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tweet_df.head(1)[0].asDict(recursive=True)\n",
    "#hashtags_df = tweet_df.select('entities.hashtags.text')\n",
    "hashtags_df = tweet_df.select(explode('entities.hashtags.text').alias('hashtag'))\n",
    "hashtags_df.groupBy('hashtag').count().orderBy('count', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
